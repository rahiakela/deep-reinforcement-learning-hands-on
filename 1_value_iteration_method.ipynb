{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-value-iteration-method.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6BYRo9nPazOk9ScRa0R3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-reinforcement-learning-hands-on/blob/chapter-5-tabular-learning-and-the-bellman-equation/1_value_iteration_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkbmvGKighWM",
        "colab_type": "text"
      },
      "source": [
        "# The value iteration method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFc4Hg2VgiLr",
        "colab_type": "text"
      },
      "source": [
        "In the simplistic example to calculate the values of the states and actions, we exploited the structure of the environment: we had no loops in transitions, so we could start from terminal states, calculate their values, and then proceed to the central state. However, just one loop in the environment builds an obstacle in our approach. \n",
        "\n",
        "Let's consider such an environment with two states:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-hands-on/transition.png?raw=1' width='800'/>\n",
        "\n",
        "We start from state s1, and the only action we can take leads us to state s2. We get the reward r = 1, and the only transition from s2 is an action, which brings us back to s1. So, the life of our agent is an infinite sequence of states `[ğ‘ 1, ğ‘ 2, ğ‘ 1, ğ‘ 2, ğ‘ 1, ğ‘ 2, ğ‘ 1, ğ‘ 2,â€¦]`.\n",
        "\n",
        "To deal with this infinity loop, we can use a discount factor: `ğ›¾ = 0.9` . Now, the question is, what are the values for both the states? The answer is not very complicated, in fact. Every transition from s1 to s2 gives us a reward of 1 and every back transition gives us 2. So, our sequence of rewards will be `[1, 2, 1, 2, 1, 2, 1, 2, â€¦.]`. As there is only one action available in every state, our agent has no choice, so we can omit the max operation in formulas (there is only one alternative).\n",
        "\n",
        "The value for every state will be equal to the infinite sum:\n",
        "\n",
        "$$ğ‘‰(ğ‘ _1) = 1 + ğ›¾ (2 + ğ›¾(1 + ğ›¾(2 + â‹¯ ))) = \\sum_{i=0}^{\\infty}1ğ›¾^{2ğ‘–} + 2ğ›¾^{2ğ‘–+1}$$\n",
        "\n",
        "$$ğ‘‰(ğ‘ _2) = 2 + ğ›¾ (1 + ğ›¾(2 + ğ›¾(1 + â‹¯ ))) = \\sum_{i=0}^{\\infty}2ğ›¾^{2ğ‘–} + 1ğ›¾^{2ğ‘–+1}$$\n",
        "\n",
        "**Strictly speaking, we can't calculate the exact values for our states, but with ğ›¾ğ›¾ = 0.9 , the contribution of every transition quickly decreases over time.**\n",
        "\n",
        "For example, after 10 steps, `ğ›¾10 = 0.910 = 0.349` , but after 100 steps, it becomes just `0.0000266`. Due to this, we can stop after 50 iterations and still get quite a precise estimation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24QZnnUdlOgP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cca58c1-5ec4-4ba4-d3e1-1824a4307d16"
      },
      "source": [
        "sum([0.9 ** (2 * i) + 2 * (0.9 ** (2 * i + 1)) for i in range(50)])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.736450674121663"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCsZJvAwlmMS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90b809e4-ae4e-48bf-a91c-4343dd9796bd"
      },
      "source": [
        "sum([2 * (0.9 ** (2 * i)) + 0.9 ** (2 * i + 1) for i in range(50)])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.262752483911719"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0jjmQ63mPI1",
        "colab_type": "text"
      },
      "source": [
        "The preceding example can be used to get the gist of a more general procedure\n",
        "called the **value iteration algorithm**. This allows us to numerically calculate the values of the states and values of the actions of **Markov decision processes (MDPs)** with known transition probabilities and rewards.\n",
        "\n",
        "The procedure (for values of the states) includes the following steps:\n",
        "\n",
        "1. Initialize the values of all states, $V_i$, to some initial value (usually zero)\n",
        "2. For every state, $s$, in the MDP, perform the Bellman update:\n",
        "$$ğ‘‰_s â† max_a \\sum_{s'}P_{a,sâ†’s'}(r_{r,s} + \\gamma V_{s'})$$\n",
        "\n",
        "3. Repeat step 2 for some large number of steps or until changes become too\n",
        "small\n",
        "\n",
        "In the case of action values (that is, Q), only minor modifications to the preceding procedure are required:\n",
        "\n",
        "1. Initialize every $Q_{s,a}$ to zero\n",
        "2. For every state, $s$, and action, $a$, in this state, perform this update:\n",
        "$$Q_{s,a} â† \\sum_{s'}P_{a,sâ†’s'}(r_{r,s} + \\gamma max_{a'} Q_{s',a'})$$\n",
        "3. Repeat step 2\n",
        "\n",
        "Okay, so that's the theory. In practice, this method has several obvious limitations.\n",
        "\n",
        "1. First of all, our state space should be discrete and small enough to perform multiple iterations over all states. This is not an issue for FrozenLake-4x4 and even for FrozenLake-8x8 (it exists in Gym as a more challenging version), but for CartPole, it's not totally clear what to do.\n",
        "\n",
        "2. The second practical problem arises from the fact that we rarely know the\n",
        "transition probability for the actions and rewards matrix. Remember the interface provided by Gym to the agent's writer: we observe the state, decide on an action, and only then do we get the next observation and reward for the transition.\n",
        "\n",
        "What we do have is just the history from the agent's interaction with the\n",
        "environment. However, in Bellman's update, we need both a reward for every\n",
        "transition and the probability of this transition. So, the obvious answer to this issue is to use our agent's experience as an estimation for both unknowns. Rewards could be used as they are. We just need to remember what reward we got on the transition from $s_0$ to $s_1$ using action a, but to estimate probabilities, we need to maintain counters for every tuple $(s_0, s_1, a)$ and normalize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KZB-wIkpQOx",
        "colab_type": "text"
      },
      "source": [
        "## Value iteration in practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2zYSNXupSe0",
        "colab_type": "text"
      },
      "source": [
        "The central data structures in this example are as follows:\n",
        "\n"
      ]
    }
  ]
}