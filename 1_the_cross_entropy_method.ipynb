{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-the-cross-entropy-method.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5M/VYgOnCJ6vd5vZyBzEk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-reinforcement-learning-hands-on/blob/chapter-4-the-cross-entropy-method/1_the_cross_entropy_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe9cwg7h6WUp",
        "colab_type": "text"
      },
      "source": [
        "#The Cross-Entropy Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSXaLw7e6dic",
        "colab_type": "text"
      },
      "source": [
        "Despite the fact that it is much less famous than other tools in the RL practitioner's toolbox, such as deep Q-network (DQN) or advantage actor-critic, the cross-entropy method has its own strengths. Firstly, the cross-entropy method is really simple, which makes it an easy method to follow. For example, its implementation on PyTorch is less than 100 lines of code.\n",
        "\n",
        "Secondly, the method has good convergence. In simple environments that don't\n",
        "require complex, multistep policies to be learned and discovered, and that have short episodes with frequent rewards, the cross-entropy method usually works very well.Of course, lots of practical problems don't fall into this category, but sometimes they do.\n",
        "\n",
        "The cross-entropy method falls into the model-free and policy-based category of\n",
        "methods. All the methods in RL can be classified into various aspects:\n",
        "* Model-free or model-based\n",
        "* Value-based or policy-based\n",
        "* On-policy or off-policy\n",
        "\n",
        "The term \"model-free\" means that the method doesn't build a model of the\n",
        "environment or reward; it just directly connects observations to actions (or values that are related to actions). In other words, the agent takes current observations and does some computations on them, and the result is the action that it should take.\n",
        "\n",
        "In contrast, model-based methods try to predict what the next observation and/or\n",
        "reward will be. Based on this prediction, the agent tries to choose the best possible action to take, very often making such predictions multiple times to look more and more steps into the future.\n",
        "\n",
        "By looking from another angle, policy-based methods directly approximate the\n",
        "policy of the agent, that is, what actions the agent should carry out at every step. The policy is usually represented by a probability distribution over the available actions.\n",
        "\n",
        "In contrast, the method could be value-based. In this case, instead of the probability of actions, the agent calculates the value of every possible action and chooses the action with the best value. Both of those families of methods are equally popular.\n",
        "\n",
        "The third important classification of methods is on-policy versus off-policy.it will be enough to explain off-policy as the ability of the method to learn on historical data (obtained by a previous version of the agent, recorded by human demonstration, or just seen by the same agent several episodes ago).\n",
        "\n",
        "So, our cross-entropy method is model-free, policy-based, and on-policy, which\n",
        "means the following:\n",
        "\n",
        "* It doesn't build any model of the environment; it just says to the agent\n",
        "what to do at every step.\n",
        "* It approximates the policy of the agent.\n",
        "* It requires fresh data obtained from the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NggDovrb46xW",
        "colab_type": "text"
      },
      "source": [
        "## Setup: Installing all required library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHNMt0JZ48vV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install torch==1.5.0+cpu torchvision==0.6.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "! pip install atari-py\n",
        "! pip install gym\n",
        "! pip install opencv-python\n",
        "! pip install pytorch-ignite\n",
        "! pip install ptan\n",
        "! pip install tensorboardX\n",
        "! pip install tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVR6LIQ75oWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4PaxXCY74dt",
        "colab_type": "text"
      },
      "source": [
        "## The cross-entropy method in practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlirxMQr75s1",
        "colab_type": "text"
      },
      "source": [
        "The cross-entropy method's description is split into two unequal parts: **practical and theoretical. The practical part is intuitive in its nature, while the theoretical explanation of why the cross-entropy method works, and what's happening, is more sophisticated.**\n",
        "\n",
        "You may remember that **the central and trickiest thing in RL is the agent, which is trying to accumulate as much total reward as possible by communicating with the environment. In practice, we follow a common machine learning (ML) approach and replace all of the complications of the agent with some kind of nonlinear trainable function, which maps the agent's input (observations from the environment) to some output.** The details of the output that this function produces may depend on a particular method or a family of methods, as described in the previous section (such as value-based versus policy-based methods). **As our cross-entropy method is policy-based, our nonlinear function (neural network (NN)) produces the policy, which basically says for every observation which action the agent should take.**\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-hands-on/high-level-rl.png?raw=1' width='800'/>\n",
        "\n",
        "In practice, **the policy is usually represented as a probability distribution over actions, which makes it very similar to a classification problem, with the amount of classes being equal to the amount of actions we can carry out.**\n",
        "\n",
        "This abstraction makes our agent very simple: it needs to pass an observation\n",
        "from the environment to the NN, get a probability distribution over actions, and\n",
        "perform random sampling using the probability distribution to get an action to\n",
        "carry out. This random sampling adds randomness to our agent, which is a good\n",
        "thing, as at the beginning of the training, when our weights are random, the agent behaves randomly. After the agent gets an action to issue, it fires the action to the environment and obtains the next observation and reward for the last action. Then the loop continues.\n",
        "\n",
        "**During the agent's lifetime, its experience is presented as episodes. Every episode is a sequence of observations that the agent has got from the environment, actions it has issued, and rewards for these actions.**\n",
        "\n",
        "Imagine that our agent has played several such episodes. For every episode, we can calculate the total reward that the agent has claimed. It can be discounted or not discounted; for simplicity, let's assume a discount factor of ùõæùõæ = 1 , which means just a sum of all local rewards for every episode. This total reward shows how good this episode was for the agent.\n",
        "\n",
        "Let's illustrate this with a diagram, which contains four episodes (note that different episodes have different values for $o_i, a_i, r_i$):\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-reinforcement-learning-hands-on/sample-episodes.png?raw=1' width='800'/>\n",
        "\n",
        "**Every cell represents the agent's step in the episode. Due to randomness in the environment and the way that the agent selects actions to take, some episodes will be better than others. The core of the cross-entropy method is to throw away bad episodes and train on better ones.**\n",
        "\n",
        "So, the steps of the method are as follows:\n",
        "1. Play N number of episodes using our current model and environment.\n",
        "2. Calculate the total reward for every episode and decide on a reward\n",
        "boundary. Usually, we use some percentile of all rewards, such as 50th\n",
        "or 70th.\n",
        "3. Throw away all episodes with a reward below the boundary.\n",
        "4. Train on the remaining \"elite\" episodes using observations as the input and\n",
        "issued actions as the desired output.\n",
        "5. Repeat from step 1 until we become satisfied with the result.\n",
        "\n",
        "With the preceding procedure, our NN learns how to repeat actions, which leads to a larger reward, constantly moving the boundary higher and higher. Despite the simplicity of this method, it works well in basic environments, it's easy to implement, and it's quite robust to hyperparameters changing, which makes it an ideal baseline method to try. \n",
        "\n",
        "Let's now apply it to our CartPole environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3jq5fv1Br98",
        "colab_type": "text"
      },
      "source": [
        "## The cross-entropy method on CartPole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmnF_LylBv31",
        "colab_type": "text"
      },
      "source": [
        "Our model's core is a one-hidden-layer NN, with rectified linear unit (ReLU) and 128 hidden neurons (which is absolutely arbitrary). Other hyperparameters are also set almost randomly and aren't tuned, as the method is robust and converges very quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0T_uTHqB89J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 128  # the count of neurons in the hidden layer\n",
        "BATCH_SIZE = 16    # the count of episodes we play on every iteration\n",
        "PERCENTILE = 70    # the percentile of episodes' total rewards that we use for \"elite\" episode filtering."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gei2q8rJCIQB",
        "colab_type": "text"
      },
      "source": [
        "We will take the 70th percentile, which means that we will leave the top 30% of episodes sorted by reward.\n",
        "\n",
        "There is nothing special about our NN; it takes a single observation from the\n",
        "environment as an input vector and outputs a number for every action we can\n",
        "perform. The output from the NN is a probability distribution over actions, so\n",
        "a straightforward way to proceed would be to include softmax nonlinearity after\n",
        "the last layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDIIXxXwCHX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self, obs_size, hidden_size, n_actions):\n",
        "    super(Network, self).__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, n_actions)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.network(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF6_RZOaI9Sa",
        "colab_type": "text"
      },
      "source": [
        "**However, in the preceding NN, we don't apply softmax to increase\n",
        "the numerical stability of the training process. Rather than calculating softmax\n",
        "(which uses exponentiation) and then calculating cross-entropy loss (which uses\n",
        "a logarithm of probabilities), we can use the PyTorch class nn.CrossEntropyLoss,\n",
        "which combines both softmax and cross-entropy in a single, more numerically\n",
        "stable expression.** \n",
        "\n",
        "**CrossEntropyLoss requires raw, unnormalized values from the\n",
        "NN (also called logits). The downside of this is that we need to remember to apply softmax every time we need to get probabilities from our NN's output.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8SPSc09Ilv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGfSXdzeKMGI",
        "colab_type": "text"
      },
      "source": [
        "Here we will define two helper classes that are named tuples from the collections package in the standard library:\n",
        "\n",
        "* **EpisodeStep**: This will be used to represent one single step that our agent\n",
        "made in the episode, and it stores the observation from the environment\n",
        "and what action the agent completed. We will use episode steps from \"elite\"\n",
        "episodes as training data.\n",
        "\n",
        "* **Episode**: This is a single episode stored as total undiscounted reward and\n",
        "a collection of EpisodeStep.\n",
        "\n",
        "Let's look at a function that generates batches with episodes:\n",
        "\n",
        "We also declare a reward counter for the current episode and its\n",
        "list of steps (the EpisodeStep objects). Then we reset our environment to obtain the first observation and create a softmax layer, which will be used to convert the NN's output to a probability distribution of actions. That's our preparations complete, so we are ready to start the environment loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2d8gUnsKLRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_batches(env, network, batch_size):\n",
        "  batch = []\n",
        "  episode_reward = 0.0\n",
        "  episode_steps = []\n",
        "  obs = env.reset()\n",
        "  softmax = nn.Softmax(dim=1)\n",
        "  '''At every iteration, we convert our current observation to a PyTorch tensor and pass it to the NN to obtain action probabilities.'''\n",
        "  while True:\n",
        "    obs_v = torch.FloatTensor([obs])\n",
        "    action_probs_v = softmax(network(obs_v))   # raw action scores feed through the softmax function to achieve nonlinearity\n",
        "    action_probs = action_probs_v.data.numpy()[0]  # get the first batch element to obtain a one-dimensional vector of action probabilities.\n",
        "    '''\n",
        "    Now that we have the probability distribution of actions, we can use it to obtain\n",
        "    the actual action for the current step by sampling this distribution using NumPy's function random.choice().\n",
        "    '''\n",
        "    action = np.random.choice(len(action_probs), p=action_probs)\n",
        "    next_obs, reward, is_done, _ = env.step(action)\n",
        "    '''After this, we will pass this action to the environment to get our next observation, our reward, and the indication of the episode ending.'''\n",
        "    episode_reward += reward\n",
        "    step = EpisodeStep(observation=obs, action=action)\n",
        "    episode_steps.append(step)\n",
        "    '''\n",
        "    The reward is added to the current episode's total reward, and our list of episode\n",
        "    steps is also extended with an (observation, action) pair. Note that we save the\n",
        "    observation that was used to choose the action, but not the observation returned by\n",
        "    the environment as a result of the action. These are the tiny, but important, details that you need to keep in mind.\n",
        "    '''\n",
        "    if is_done:  # handle the situation when the current episode is over\n",
        "      e = Episode(reward=episode_reward, steps=episode_steps)\n",
        "      batch.append(e)\n",
        "      episode_reward = 0.0\n",
        "      episode_steps = []      # reset total reward accumulator and clean the list of steps\n",
        "      next_obs = env.reset()  # reset environment to start over.\n",
        "      '''\n",
        "      In case batch has reached the desired count of episodes,return it to the caller for processing using yield. Our function is a generator, so every time \n",
        "      the yield operator is executed, the control is transferred to the outer iteration loop and then continues after the yield line.\n",
        "      '''\n",
        "      if len(batch) == batch_size:\n",
        "        yield batch\n",
        "        batch = []\n",
        "    obs = next_obs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnytEfIRUf2-",
        "colab_type": "text"
      },
      "source": [
        "The last, but very important, step in our loop is to assign an observation obtained from the environment to our current observation variable. After that, everything repeats infinitely‚Äîwe pass the observation to the NN, sample the action to perform, ask the environment to process the action, and remember the result of this processing.\n",
        "\n",
        "**One very important fact to understand in this function logic is that the training of our NN and the generation of our episodes are performed at the same time.** They are not completely in parallel, but every time our loop accumulates enough episodes (16), it passes control to this function caller, which is supposed to train the NN using gradient descent. So, when yield is returned, the NN will have different, slightly better (we hope) behavior.\n",
        "\n",
        "**We don't need to explore proper synchronization, as our training and data gathering activities are performed at the same thread of execution, but you need to understand those constant jumps from NN training to its utilization.**\n",
        "\n",
        "Okay, now we need to define yet another function and then we will be ready to\n",
        "switch to the training loop.\n",
        "\n",
        "This function is at the core of the cross-entropy method‚Äîfrom the given batch\n",
        "of episodes and percentile value, it calculates a boundary reward, which is used\n",
        "to filter \"elite\" episodes to train on. \n",
        "\n",
        "To obtain the boundary reward, we will use\n",
        "NumPy's percentile function, which, from the list of values and the desired\n",
        "percentile, calculates the percentile's value. Then, we will calculate the mean\n",
        "reward, which is used only for monitoring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7qtL19jUetY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_batch(batch, percentile):\n",
        "  rewards = list(map(lambda s: s.reward, batch))\n",
        "  reward_bound = np.percentile(rewards, percentile)\n",
        "  reward_mean = float(np.mean(rewards))\n",
        "  \n",
        "  train_obs = []\n",
        "  train_action = []\n",
        "  for reward, steps in batch:\n",
        "    if reward < reward_bound:\n",
        "      continue\n",
        "    train_obs.extend(map(lambda step: step.observation, steps))\n",
        "    train_action.extend(map(lambda step: step.action, steps))\n",
        "  '''\n",
        "  Next, we will filter off our episodes. For every episode in the batch, we will check\n",
        "  that the episode has a higher total reward than our boundary and if it has, we will\n",
        "  populate lists of observations and actions that we will train on.\n",
        "  '''\n",
        "  train_obs_v = torch.FloatTensor(train_obs)\n",
        "  train_action_v = torch.FloatTensor(train_action)\n",
        "\n",
        "  return train_obs_v, train_action_v, reward_bound, reward_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxQXnJUXX4SY",
        "colab_type": "text"
      },
      "source": [
        "As the final step of the function, we will convert our observations and actions from \"elite\" episodes into tensors, and return a tuple of four: observations, actions, the boundary of reward, and the mean reward. The last two values will be used only to write them into TensorBoard to check the performance of our agent.\n",
        "\n",
        "Now, the final chunk of code that glues everything together, and mostly consists\n",
        "of the training loop, is as follows:\n",
        "\n",
        "In the beginning, we create all the required objects: the environment, our NN, the objective function, the optimizer, and the summary writer for TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7U0BenzX3iG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "# env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
        "obs_size = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "network = Network(obs_size, HIDDEN_SIZE, n_actions)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=network.parameters(), lr=0.01)\n",
        "writer = SummaryWriter(comment='-cartpole')\n",
        "\n",
        "# The commented line creates a monitor to write videos of your agent's performance.\n",
        "for iter_no, batch in enumerate(iterate_batches(env, network, BATCH_SIZE)):\n",
        "  obs_v, action_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)  # perform filtering of the \"elite\" episodes using the filter_batch function.\n",
        "  optimizer.zero_grad()   # zero gradients of NN and pass observations to the NN, obtaining its action scores.\n",
        "  action_scores_v = network(obs_v)   \n",
        "  # scores are passed to the objective function, which will calculate cross-entropy between the NN output and the actions that the agent took.\n",
        "  loss_v = objective(action_scores_v, action_v)\n",
        "  loss_v.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  '''\n",
        "  The idea of this is to reinforce our NN to carry out those \"elite\" actions that have led to good rewards. Then, we calculate gradients\n",
        "  on the loss and ask the optimizer to adjust our NN.\n",
        "  '''\n",
        "\n",
        "  # We also write the same values to TensorBoard, to get a nice chart of the agent's learning performance.\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}